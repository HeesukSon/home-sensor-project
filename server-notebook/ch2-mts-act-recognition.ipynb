{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 1\n",
    "el_len = 50\n",
    "n_fold = 10\n",
    "\n",
    "act_map = {'work':0, \n",
    "           'eating':1, \n",
    "           'toilet':2, \n",
    "           'fitness':3, \n",
    "           'sleep':4, \n",
    "           'personal_hygiene':5, \n",
    "           'shower':6, \n",
    "           'relax':7, \n",
    "           'cooking':8, \n",
    "           'phonecall':9, \n",
    "           'leave_home':10, \n",
    "           'null':11}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTS data loading and its transformation to (X, Y) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>snd11_roadside</th>\n",
       "      <th>snd12_bed</th>\n",
       "      <th>snd21_kitchen</th>\n",
       "      <th>snd22_desk</th>\n",
       "      <th>tmp1</th>\n",
       "      <th>hmd1</th>\n",
       "      <th>light1</th>\n",
       "      <th>mot1</th>\n",
       "      <th>tmp2</th>\n",
       "      <th>hmd2</th>\n",
       "      <th>light2</th>\n",
       "      <th>mot2</th>\n",
       "      <th>weekend</th>\n",
       "      <th>hour</th>\n",
       "      <th>null-1</th>\n",
       "      <th>null-2</th>\n",
       "      <th>act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-05-07 22:19:54.111575-04:00</td>\n",
       "      <td>0.112980</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>0.125199</td>\n",
       "      <td>0.115151</td>\n",
       "      <td>0.579129</td>\n",
       "      <td>0.300179</td>\n",
       "      <td>0.06409</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.585029</td>\n",
       "      <td>0.302947</td>\n",
       "      <td>0.03906</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-07 22:19:57.473695-04:00</td>\n",
       "      <td>0.113832</td>\n",
       "      <td>0.090506</td>\n",
       "      <td>0.125769</td>\n",
       "      <td>0.127575</td>\n",
       "      <td>0.579029</td>\n",
       "      <td>0.300684</td>\n",
       "      <td>0.06415</td>\n",
       "      <td>0.248947</td>\n",
       "      <td>0.585029</td>\n",
       "      <td>0.302947</td>\n",
       "      <td>0.03903</td>\n",
       "      <td>0.244583</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-05-07 22:19:58.590782-04:00</td>\n",
       "      <td>0.114485</td>\n",
       "      <td>0.089217</td>\n",
       "      <td>0.118430</td>\n",
       "      <td>0.119727</td>\n",
       "      <td>0.579129</td>\n",
       "      <td>0.300684</td>\n",
       "      <td>0.06412</td>\n",
       "      <td>0.247681</td>\n",
       "      <td>0.585029</td>\n",
       "      <td>0.302568</td>\n",
       "      <td>0.03906</td>\n",
       "      <td>0.250595</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-05-07 22:20:00.861969-04:00</td>\n",
       "      <td>0.113706</td>\n",
       "      <td>0.101555</td>\n",
       "      <td>0.122844</td>\n",
       "      <td>0.126247</td>\n",
       "      <td>0.578929</td>\n",
       "      <td>0.300926</td>\n",
       "      <td>0.06415</td>\n",
       "      <td>0.248154</td>\n",
       "      <td>0.585129</td>\n",
       "      <td>0.302705</td>\n",
       "      <td>0.03906</td>\n",
       "      <td>0.242569</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-05-07 22:20:06.581213-04:00</td>\n",
       "      <td>0.115635</td>\n",
       "      <td>0.082554</td>\n",
       "      <td>0.133055</td>\n",
       "      <td>0.122825</td>\n",
       "      <td>0.579029</td>\n",
       "      <td>0.300926</td>\n",
       "      <td>0.06409</td>\n",
       "      <td>0.248978</td>\n",
       "      <td>0.585029</td>\n",
       "      <td>0.302389</td>\n",
       "      <td>0.03903</td>\n",
       "      <td>0.248520</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                              time  snd11_roadside  snd12_bed  \\\n",
       "0           0  2020-05-07 22:19:54.111575-04:00        0.112980   0.090066   \n",
       "1           1  2020-05-07 22:19:57.473695-04:00        0.113832   0.090506   \n",
       "2           2  2020-05-07 22:19:58.590782-04:00        0.114485   0.089217   \n",
       "3           3  2020-05-07 22:20:00.861969-04:00        0.113706   0.101555   \n",
       "4           4  2020-05-07 22:20:06.581213-04:00        0.115635   0.082554   \n",
       "\n",
       "   snd21_kitchen  snd22_desk      tmp1      hmd1   light1      mot1      tmp2  \\\n",
       "0       0.125199    0.115151  0.579129  0.300179  0.06409  0.249023  0.585029   \n",
       "1       0.125769    0.127575  0.579029  0.300684  0.06415  0.248947  0.585029   \n",
       "2       0.118430    0.119727  0.579129  0.300684  0.06412  0.247681  0.585029   \n",
       "3       0.122844    0.126247  0.578929  0.300926  0.06415  0.248154  0.585129   \n",
       "4       0.133055    0.122825  0.579029  0.300926  0.06409  0.248978  0.585029   \n",
       "\n",
       "       hmd2   light2      mot2  weekend      hour  null-1  null-2  act  \n",
       "0  0.302947  0.03906  0.239700        0  0.916667       0       0    0  \n",
       "1  0.302947  0.03903  0.244583        0  0.916667       0       0    0  \n",
       "2  0.302568  0.03906  0.250595        0  0.916667       0       0    0  \n",
       "3  0.302705  0.03906  0.242569        0  0.916667       0       0    0  \n",
       "4  0.302389  0.03903  0.248520        0  0.916667       0       0    0  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('act_mts_data.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 203531, list, 203531)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_x = data_df[['snd11_roadside', 'snd12_bed', 'snd21_kitchen', 'snd22_desk', 'tmp1', 'hmd1', 'light1', 'mot1', 'tmp2', 'hmd2', 'light2', 'mot2', 'weekend', 'hour', 'null-1', 'null-2']]\n",
    "subset_y = data_df['act']\n",
    "dataset_features = subset_x.values.tolist()\n",
    "dataset_labels = subset_y.tolist()\n",
    "\n",
    "type(dataset_labels), len(dataset_labels), type(dataset_features), len(dataset_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping input features to 50x4x4 (C x H x W)\n",
    "* To make make an input feqture of shape (50x4x4) after ToTensor transformation later, 4x4x50 matrix should be created here.\n",
    "* By adjusting the data sampling rate, each data element can cover longer period of time (e.g., 1/2 data sampling results in 2 mins of MTS data). \n",
    "* The majority voting can be used to decide each cubic data point's label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((203500, 16), (203500,), numpy.ndarray, 16)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_arr = np.asarray(dataset_features)\n",
    "labels_arr = np.asarray(dataset_labels)\n",
    "\n",
    "residual = len(dataset_labels)%el_len\n",
    "\n",
    "features_arr = features_arr[:-residual]\n",
    "labels_arr = labels_arr[:-residual]\n",
    "\n",
    "features_arr.shape, labels_arr.shape, type(features_arr[0]), len(features_arr[0])\n",
    "#labels_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping feature array to 50x4x4 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 4, 4, 50)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_arr = features_arr.reshape(int(len(features_arr)/el_len), el_len, 4, 4)\n",
    "features_arr = np.transpose(features_arr, (0,2,3,1))\n",
    "features_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeezing labels by means of the most frequent values in each 50-length array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]), (4070,))"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_arr = labels_arr.reshape(int(len(labels_arr)/el_len), el_len)\n",
    "labels_arr = [np.bincount(x).argmax() for x in labels_arr]\n",
    "labels_arr = np.asarray(labels_arr)\n",
    "labels_arr, labels_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4070, 2), (2,), (4, 4, 50), ())"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tuple(zip(features_arr, labels_arr))\n",
    "dataset = np.asarray(dataset)\n",
    "dataset.shape, dataset[0].shape, dataset[0][0].shape, dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the dataset into training and validation datasets\n",
    "Considering the characteristics of human activities in a smart home, splitting the dataset in an arbitrary basis may not be an optimal choice with respect to the recognition accuracy. However, the main goal of this experiment is to see if n-minute-long MTS data can represent any meaningful patterns of human activities. Therefore, we split the dataset arbitrarily in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3663, 2), (407, 2))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data = dataset.shape[0]\n",
    "n_val = int(n_data * 0.1)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_data)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "dataset_train = dataset[train_indices]\n",
    "dataset_val = dataset[val_indices]\n",
    "\n",
    "dataset_train.shape, dataset_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data to tensor\n",
    "By definition, **ToTensor** transforms the image or numpy.ndarray to a tensor with range [0,1]. As the result, the input shape **(H x W x C)** is transformed into the output shape **(C x H x W)** in the range of [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "dataset_train_t = [(to_tensor(mts).to(torch.float32), label) for mts, label in dataset_train]\n",
    "dataset_val_t = [(to_tensor(mts).to(torch.float32), label) for mts, label in dataset_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple PyTorch CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset_train_t, batch_size = 64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val_t, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(datetime.now(), epoch, float(loss_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_channel = el_len\n",
    "        self.conv1 = nn.Conv2d(self.n_channel, self.n_channel*3, kernel_size=3, padding=1) # IN: 50x4x4, OUT: 150x4x4\n",
    "        self.conv1_bn = nn.BatchNorm2d(self.n_channel*3)\n",
    "        # maxpool2d, IN: 50x4x4, OUT: 50x2x2\n",
    "        self.conv2 = nn.Conv2d(self.n_channel*3, self.n_channel*2, kernel_size=2, padding=1) # IN: 150x2x2, OUT: 100x1x1\n",
    "        self.conv2_bn = nn.BatchNorm2d(self.n_channel*2)\n",
    "        # maxpool2d, IN: 50x3x3, OUT: 50x1x1\n",
    "        self.fc1 = nn.Linear(1*1*self.n_channel*2, 32)\n",
    "        self.fc2 = nn.Linear(32,len(act_map.keys()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(F.relu(self.conv1_bn(self.conv1(x))), 2)\n",
    "        out = F.max_pool2d(F.relu(self.conv2_bn(self.conv2(out))), 2)\n",
    "        out = out.view(-1, 1*1*self.n_channel*2) \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-13 22:26:37.118826 Epoch 1, Training loss 99.1751571893692\n",
      "2020-05-13 22:26:47.102250 Epoch 10, Training loss 0.48210770823061466\n",
      "2020-05-13 22:26:58.131146 Epoch 20, Training loss 0.18653730326332152\n",
      "2020-05-13 22:27:09.522935 Epoch 30, Training loss 0.11183002323377877\n",
      "2020-05-13 22:27:21.128023 Epoch 40, Training loss 0.07862311461940408\n",
      "2020-05-13 22:27:33.415424 Epoch 50, Training loss 0.05938897019950673\n",
      "2020-05-13 22:27:45.833668 Epoch 60, Training loss 0.04791862261481583\n",
      "2020-05-13 22:27:59.086499 Epoch 70, Training loss 0.03998408810002729\n",
      "2020-05-13 22:28:11.567491 Epoch 80, Training loss 0.03413496620487422\n",
      "2020-05-13 22:28:24.417799 Epoch 90, Training loss 0.02972908320953138\n",
      "2020-05-13 22:28:37.106330 Epoch 100, Training loss 0.026218993036309257\n",
      "2020-05-13 22:28:49.811969 Epoch 110, Training loss 0.02350875682896003\n",
      "2020-05-13 22:29:03.034967 Epoch 120, Training loss 0.02120962250046432\n",
      "2020-05-13 22:29:16.300005 Epoch 130, Training loss 0.019386974279768765\n",
      "2020-05-13 22:29:29.435312 Epoch 140, Training loss 0.017802529386244714\n",
      "2020-05-13 22:29:42.119483 Epoch 150, Training loss 0.016396950188209303\n",
      "2020-05-13 22:29:54.734954 Epoch 160, Training loss 0.015259592270012945\n",
      "2020-05-13 22:30:07.382155 Epoch 170, Training loss 0.014233349575079046\n",
      "2020-05-13 22:30:21.014322 Epoch 180, Training loss 0.013316440119524486\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(n_epochs = 250, optimizer = optimizer, model = model.train(), loss_fn = loss_fn, train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.796791\n",
      "Accuracy: 0.695652\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset_train_t, batch_size = 64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val_t, batch_size = 64, shuffle=False)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "for loader in [train_loader, val_loader]:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "    \n",
    "    print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the answers and Compare them with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
